{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaSfWOtEL3jp"
      },
      "source": [
        "**Import the Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "112kFMUDNLO1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-21 20:33:11.673577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-21 20:33:12.428896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (4.0.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myecanlee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "#Import the libraries\n",
        "#from Starscream import TransUNet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from glob import glob\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import h5py\n",
        "\n",
        "# Import the machine learning libraries\n",
        "import sklearn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.cuda import amp\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
        "import tensorflow as tf\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from timm.models.vision_transformer import _cfg\n",
        "from einops import rearrange\n",
        "\n",
        "# For exponential moving average\n",
        "from ema_pytorch import EMA\n",
        "\n",
        "# For cool stuff\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "# Import the monitoring libraries\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# Import the warnings, suppress all the warnings messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For debug\n",
        "# For RAM usage\n",
        "import gc # automatic garbage collector\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"shockwave.ipynb\"\n",
        "# In case we have a run out of memory problem\n",
        "\n",
        "# Import the augmemtation libraries\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# REMEBER TO MOVE THIS BEFORE PUBLISHING!\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(torch.cuda.device_count())\n",
        "\n",
        "#if torch.cuda.device_count() > 1:\n",
        "    #print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    #model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEZt4vHRMA_A"
      },
      "source": [
        "**Define the Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YpEuBry8L9g2"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "\n",
        "    # special settings for wandb\n",
        "    wandb = True\n",
        "    comment = 'Transformer-Unet baseline, Efficient Attention'\n",
        "\n",
        "    verbose = 1\n",
        "\n",
        "    seed = 42\n",
        "    fold = 10\n",
        "    selected_folds = [0,1,2,3,4,5,6,7,8,9]\n",
        "    image_size = (512,512) # 224\n",
        "\n",
        "    P1 = 1.0\n",
        "    P2 = 0.5\n",
        "    P3 = 1.0\n",
        "\n",
        "    train_batch_size = 14\n",
        "    valid_batch_size = 32\n",
        "    test_batch_size = 32\n",
        "    drop_reminder = False\n",
        "    epochs = 50\n",
        "    train_num_workers = 0\n",
        "    test_num_workers = 0\n",
        "    \n",
        "    anonymous = None\n",
        "    \n",
        "    loss = 'dice_loss'\n",
        "    optimizer = 'AdamW'\n",
        "    # learning rate scheduler would be chose during the training phase\n",
        "    lr_scheduler = 'CosineAnnealingLR' # 'ReduceLROnPlateau', 'CosineAnnealingWarmRestarts', 'ExpotentialLR'\n",
        "    lr_min = 1e-6 # for CosineAnnealingWarmRestarts and CosineAnnealingLR\n",
        "    T_max = int(epochs*0.7)\n",
        "    T_0 = 0\n",
        "    patience = 5 # for ReduceLROnPlateau\n",
        "    mode = 'min' # for ReduceLROnPlateau\n",
        "    threshod = 0.0001 # for ReduceLROnPlateau\n",
        "    gamma = 0.5 # for ExponentialLR\n",
        "    weight_decay = 1e-5\n",
        "    \n",
        "    DEBUG = False\n",
        "    # segmentation class large bowel, small bowel and stomach(Background)\n",
        "    num_classes = 1\n",
        "\n",
        "    # training device\n",
        "    # this would be trained on a single GPU 4090\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    n_accumulate  = 1 # gradient accumulation steps\n",
        "    comment = 'TransUNet-Tested version 512-512'\n",
        "    model_name = 'TransUNet Test Version'\n",
        "    MAX_PIXEL_VALUE = 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxzXkmHcMP53"
      },
      "source": [
        "**Reproducibility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "odRBuQBrMNBX"
      },
      "outputs": [],
      "source": [
        "# Define the reproduviability seed\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # set the torch backends for cudnn\n",
        "    torch.backends.cudnn.determinitic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # set the python hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print('Everything is set up for the reproducibility.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3utwVlaVMnG3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Everything is set up for the reproducibility.\n"
          ]
        }
      ],
      "source": [
        "# Set reproducibility\n",
        "seed_everything(CFG.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4XOjiVMehA"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuTcClnRI0S"
      },
      "source": [
        "**Augmentation Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRbk0ACGqTrb"
      },
      "outputs": [],
      "source": [
        "# https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
        "# There are 3 probability inside the albumentation library\n",
        "# They are called p1, p2, p3\n",
        "# P1: the probability of the all the images will be augmented\n",
        "# P2: the probability of the image will be augmented in a very specific way\n",
        "# P3: the probability of the image will be augmented by 'OneOf' function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FAcNhUxrPVF5"
      },
      "outputs": [],
      "source": [
        "P1 = CFG.P1\n",
        "P2 = CFG.P2\n",
        "P3 = CFG.P3\n",
        "\n",
        "# Mean and validation values from imagenet for normalization\n",
        "# mean = (0.485, 0.456, 0.406)\n",
        "# std = (0.229, 0.224, 0.225)\n",
        "mean = (0.5,)\n",
        "std = (0.5,)\n",
        "class AlbumentationsTransform:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = A.Compose(transforms)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        augmented = self.transforms(image=image, mask=label)\n",
        "        image, label = augmented['image'], augmented['mask']\n",
        "        return {'image': image, 'label': label}\n",
        "\n",
        "train_transform = AlbumentationsTransform([\n",
        "        # Resize the image to 224x224 using nearest neighbor interpolation\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        # A.Resize(224, 224, interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "        # Apply horizontal and vertical flips with a probability of PROB_FLIP_ROTATE\n",
        "        A.HorizontalFlip(p = P2),\n",
        "        A.VerticalFlip(p = P2),\n",
        "\n",
        "        # Randomly rotate the image by 90 degrees with a probability of PROB_FLIP_ROTATE\n",
        "        A.RandomRotate90(p = P2),\n",
        "\n",
        "        # Transpose the image with a probability of PROB_FLIP_ROTATE\n",
        "        A.Transpose(p = P2),\n",
        "\n",
        "        # Apply one of the following distortions with a combined probability of PROB_DISTORT\n",
        "        A.OneOf([\n",
        "            A.ElasticTransform(alpha = 120, sigma = 120 * 0.05, alpha_affine = 120 * 0.03, p=0.5),\n",
        "            A.GridDistortion(p = 0.5),\n",
        "            A.OpticalDistortion(distort_limit = 2, shift_limit = 0.5, p = P3)\n",
        "        ], p = P1),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "        # Resize the image to 256x256 using nearest neighbor interpolation for validation\n",
        "valid_transform = AlbumentationsTransform([\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2()\n",
        "        # A.Resize(224, 224, interpolation = cv2.INTER_NEAREST)\n",
        "    ])\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Synapse_dataset(Dataset):\n",
        "    def __init__(self, base_dir, samples=None, list_dir=None, split=\"train\", transform=None):\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        if samples:\n",
        "            self.sample_list = samples\n",
        "        else:\n",
        "            self.sample_list = open(os.path.join(list_dir, split+'.txt')).readlines()\n",
        "        self.data_dir = base_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        slice_name = self.sample_list[idx].strip('\\n')\n",
        "        if self.split != \"test\":\n",
        "            data_path = os.path.join(self.data_dir, slice_name+'.npz')\n",
        "            data = np.load(data_path)\n",
        "            image, label = data['image'], data['label']\n",
        "        else:\n",
        "            filepath = self.data_dir + \"/{}.npy.h5\".format(slice_name)\n",
        "            data = h5py.File(filepath)\n",
        "            image, label = data['image'][:], data['label'][:]\n",
        "\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        sample['case_name'] = slice_name\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0nHsdRjvYk5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_synapse_loaders(base_dir: str, list_dir: str, debug: bool = False) -> tuple:\n",
        "    \"\"\"\n",
        "    Prepare training, validation, and test data loaders.\n",
        "\n",
        "    Parameters:\n",
        "    - base_dir: Base directory for the dataset.\n",
        "    - list_dir: Directory containing the list files.\n",
        "    - debug: Whether to run in debug mode (optional, default is False).\n",
        "\n",
        "    Returns:\n",
        "    - Tuple containing the training, validation, and test data loaders.\n",
        "    \"\"\"\n",
        "    # Load the entire list of training samples\n",
        "    # all_samples = open(os.path.join(list_dir, 'train.txt')).readlines()\n",
        "\n",
        "    # Split the samples into training and validation sets (80-20 split)\n",
        "    # train_samples, valid_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets using the split samples\n",
        "    train_dataset = Synapse_dataset(base_dir=base_dir, list_dir=list_dir, split=\"train\", transform=train_transform)\n",
        "    # valid_dataset = Synapse_dataset(base_dir=base_dir, samples=valid_samples, transform=valid_transform)\n",
        "    test_dataset = Synapse_dataset(base_dir=base_dir, list_dir=list_dir, split=\"test\", transform=valid_transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_bs = CFG.train_batch_size if not debug else 4\n",
        "    # valid_bs = CFG.valid_batch_size if not debug else 4\n",
        "    test_bs = CFG.valid_batch_size if not debug else 4\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_bs,\n",
        "                              num_workers=0, shuffle=True, pin_memory=True, drop_last=False)\n",
        "    # valid_loader = DataLoader(valid_dataset, batch_size=valid_bs,\n",
        "    #                          num_workers=0, shuffle=False, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=test_bs,\n",
        "                             num_workers=0, shuffle=False, pin_memory=True)\n",
        "\n",
        "    return train_loader,  test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJrbkkx0cUtt"
      },
      "source": [
        "**Get train and test dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader, test_loader = prepare_synapse_loaders(\n",
        "    base_dir=r\"project_TransUNet/data/Synapse/train_npz\", \n",
        "    list_dir=r\"project_TransUNet/TransUNet/lists/lists_Synapse\", \n",
        "    debug=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO-MA8CRdAt9"
      },
      "source": [
        "**Visualize Some Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYMZXxazbOo1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Get images and labels from the batch\n",
        "images = batch['image']\n",
        "print(images.shape)\n",
        "labels = batch['label']\n",
        "\n",
        "# Convert images to a PyTorch tensor\n",
        "images_tensor = torch.tensor(images)\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# Iterate over the images and print them one by one\n",
        "for i in range(images_tensor.size(0)):\n",
        "    image = images_tensor[i]\n",
        "    image_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    plt.imshow(image_np,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "# Iterate over the images and print them one by one\n",
        "for i in range(labels_tensor.size(0)):\n",
        "    image_1 = labels_tensor[i]\n",
        "    image_11 = image_1.numpy()\n",
        "\n",
        "    plt.imshow(image_11,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOZnJ6pCc6FN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for i_batch, sampled_batch in enumerate(valid_loader):\n",
        "    images = sampled_batch['image']\n",
        "\n",
        "    # Convert images to a PyTorch tensor\n",
        "    images_tensor = torch.tensor(images)\n",
        "\n",
        "    # Iterate over the images and print them one by one\n",
        "    for i in range(images_tensor.size(0)):\n",
        "        image = images_tensor[i]\n",
        "\n",
        "        # Select a specific channel (e.g., the first channel)\n",
        "        channel_image = image[0]  # Select the first channel\n",
        "\n",
        "        # Convert channel image to a NumPy array\n",
        "        image_np = channel_image.numpy()\n",
        "\n",
        "        # Display the image using Matplotlib\n",
        "        plt.imshow(image_np,cmap='gray')  # Assuming it's a grayscale image\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Show all images in this batch\n",
        "    plt.show()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Define the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os.path import join as pjoin\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from einops import rearrange\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, factor=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        expansion = int(out_channels * (factor / 64))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, expansion, kernel_size=1, stride=1, bias=False)\n",
        "        self.norm1 = nn.BatchNorm2d(expansion)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(expansion, expansion, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
        "        self.norm2 = nn.BatchNorm2d(expansion)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(expansion, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_down = self.downsample(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = x + x_down\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the vit model from torchvision\n",
        "\n",
        "from torchvision.models import vit_b_16\n",
        "from torchvision.models import ViT_B_16_Weights\n",
        "\n",
        "torch_vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "class PretrainedViTEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PretrainedViTEncoder, self).__init__()\n",
        "        self.vit = torch_vit\n",
        "        self.project = self.vit.conv_proj\n",
        "        # Remove the last 4 encoder layers to keep only the first 8\n",
        "        # self.vit.encoder.layers = self.vit.encoder.layers[:1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project(x)\n",
        "        x = rearrange(x, \"b c w h -> b (w h) c\")\n",
        "        # x = self.vit.encoder(x)\n",
        "        # print(\"After ViT Encoder:\", x.shape)\n",
        "        # print(self.project)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocusedLinearAttention(nn.Module):\n",
        "    def __init__(self, dim, num_patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.2, proj_drop=0.2,\n",
        "                 focusing_factor=3, kernel_size=5):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        #self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "\n",
        "        self.focusing_factor = focusing_factor\n",
        "        # Depthwise Convolution, padding = kernel_size//2 to make sure the image with the same size after convolution\n",
        "        self.dwc = nn.Conv2d(in_channels=head_dim, out_channels=head_dim, kernel_size=kernel_size,\n",
        "                             groups=head_dim, padding=kernel_size // 2)\n",
        "        self.scale = nn.Parameter(torch.zeros(size=(1, 1, dim)))\n",
        "        # Remeber to modify the positional encoding to adjust to the new length\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(size=(1, num_patches, dim)))\n",
        "        # print('Linear Attention  f{} kernel{}'.\n",
        "              #format(focusing_factor, kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        q = self.q(x)\n",
        "\n",
        "        # Split the k and v matrixs here\n",
        "        # kv = self.kv(x).reshape(B, -1, 2, C).permute(2, 0, 1, 3)\n",
        "        # k, v = kv[0], kv[1]\n",
        "\n",
        "        # The shape of the k,v now would be (B, N, C)\n",
        "        k = k + self.positional_encoding\n",
        "        focusing_factor = self.focusing_factor\n",
        "        kernel_function = nn.ReLU()\n",
        "        scale = nn.Softplus()(self.scale)\n",
        "        q = kernel_function(q) + 1e-6\n",
        "        k = kernel_function(k) + 1e-6\n",
        "        q = q / scale\n",
        "        k = k / scale\n",
        "        q_norm = q.norm(dim=-1, keepdim=True)\n",
        "        k_norm = k.norm(dim=-1, keepdim=True)\n",
        "        q = q ** focusing_factor\n",
        "        k = k ** focusing_factor\n",
        "        q = (q / q.norm(dim=-1, keepdim=True)) * q_norm\n",
        "        k = (k / k.norm(dim=-1, keepdim=True)) * k_norm\n",
        "\n",
        "        # Rearrange into multi-head dimension, each head will have C/H dimensions\n",
        "        q, k, v = (rearrange(x, \"b n (h c) -> (b h) n c\", h=self.num_heads) for x in [q, k, v])\n",
        "        i, j, c, d = q.shape[-2], k.shape[-2], k.shape[-1], v.shape[-1]\n",
        "        # print(i, j, c, d)\n",
        "        # print(q.shape)\n",
        "        # print(k.shape)\n",
        "        # print(v.shape)\n",
        "        # print(i * j * (c + d))\n",
        "        # print(c * d * (i + j))\n",
        "        z = 1 / (torch.einsum(\"b i c, b c -> b i\", q, k.sum(dim=1)) + 1e-6)\n",
        "        # Using Linear Attention Mechanism here to get O(N) complexity\n",
        "        if i * j * (c + d) > c * d * (i + j):\n",
        "            kv = torch.einsum(\"b j c, b j d -> b c d\", k, v)\n",
        "            x = torch.einsum(\"b i c, b c d, b i -> b i d\", q, kv, z)\n",
        "        else:\n",
        "            qk = torch.einsum(\"b i c, b j c -> b i j\", q, k)\n",
        "            x = torch.einsum(\"b i j, b j d, b i -> b i d\", qk, v, z)\n",
        "\n",
        "        num = int(v.shape[1] ** 0.5)\n",
        "        feature_map = rearrange(v, \"b (w h) c -> b c w h\", w=num, h=num)\n",
        "        # Expanding the attention matrix rank from d to N to expand the expressing power of the model\n",
        "        feature_map = rearrange(self.dwc(feature_map), \"b c w h -> b (w h) c\")\n",
        "        # Adding the expanding version feature map back into the input\n",
        "        x = x + feature_map\n",
        "        x = rearrange(x, \"(b h) n c -> b n (h c)\", h=self.num_heads)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp_layers = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp_layers(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from performer import Attention\n",
        "from longformer import Long2DSCSelfAttention\n",
        "from linearformer import LinearAttention\n",
        "from nystroem import NystromAttention\n",
        "from efficient_attention import EfficientAttention #\n",
        "from focused_linear_attention import FocusedLinearAttention\n",
        "from linformer import LinformerAttention #\n",
        "\n",
        "class Linear_ViT_EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_patches, num_heads = 8, qkv_bias = False, qk_scale = None, attn_drop = 0, proj_drop = 0,\n",
        "                 focusing_factor = 3, kernel_size = 5, mlp_dim = 2048):\n",
        "        super().__init__()\n",
        "\n",
        "# Uncomment the attention that you want to use\n",
        "\n",
        "        self.attention = Attention(dim = dim)\n",
        "#        self.attention = Long2DSCSelfAttention(dim = dim, num_patches = num_patches) # contains an error\n",
        "#        self.attention = LinearAttention(dim = dim, num_patches = 1024)\n",
        "        '''        \n",
        "            self.attention = NystromAttention(\n",
        "            dim = 768,\n",
        "            dim_head = 96,\n",
        "            heads = 8,\n",
        "            num_landmarks = 256,    # number of landmarks\n",
        "            pinv_iterations = 6,    # number of moore-penrose iterations for approximating pinverse. 6 was recommended by the paper\n",
        "            residual = True         # whether to do an extra residual with the value or not. supposedly faster convergence if turned on\n",
        "        )\n",
        "        '''\n",
        "#        self.attention = EfficientAttention(dim = dim, patches = num_patches)\n",
        "#        self.attention = FocusedLinearAttention(dim, num_patches, num_heads = 8, qkv_bias = False,\n",
        "#                                                           qk_scale = None, attn_drop = 0, proj_drop = 0, focusing_factor = 3, kernel_size = 5)\n",
        "#        self.attention = LinformerAttention(dim)\n",
        "\n",
        "        # FFN bottleneck layer with expanding then shrinking dimension\n",
        "        self.mlp = MLP(dim, mlp_dim)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(768)\n",
        "        self.layer_norm2 = nn.LayerNorm(768)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Assuming x is of shape (batch_size, num_patches, dim)\n",
        "      # and you've flattened a HxW grid of patches\n",
        "        H = W = int(math.sqrt(x.shape[1]))\n",
        "\n",
        "        #ALERT: Change\n",
        "        _x = self.attention(x)\n",
        "        _x = self.dropout(_x)\n",
        "        x = x + _x\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        _x = self.mlp(x)\n",
        "        x = x + _x\n",
        "        x = self.layer_norm2(x)\n",
        "        return x\n",
        "\n",
        "# Encoder Block for pure focused linear attention vision transformer\n",
        "# The block_num would be only 4 since the first eight encoder block would use the pretrained ViT weights based on vanilla attention\n",
        "class Linear_ViT_Encoder(nn.Module):\n",
        "    def __init__(self, dim, num_patches, num_heads = 8, qkv_bias = False, qk_scale = None, attn_drop = 0, proj_drop = 0,\n",
        "                 focusing_factor = 3, kernel_size = 5, block_num=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_blocks = nn.ModuleList(\n",
        "            [Linear_ViT_EncoderBlock(dim, num_patches, num_heads = 8, qkv_bias = False, qk_scale = None, attn_drop = 0, proj_drop = 0,\n",
        "                 focusing_factor = 3, kernel_size = 5) for _ in range(block_num)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_block in self.layer_blocks:\n",
        "            x = layer_block(x)\n",
        "        #print(\"After Linear ViT Encoder:\", x.shape)\n",
        "        #!!!!\n",
        "        # x = rearrange(x, \"b (e f) c -> b c e f\", e=2, f=2) #32, 32 for real\n",
        "        # print('Ready to be put into Decoder!', x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransUNetEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, model_dim, num_patches, num_heads = 8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = model_dim // num_heads\n",
        "        self.num_patches = num_patches\n",
        "        #self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
        "        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
        "        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
        "\n",
        "        self.pretrained_vit = PretrainedViTEncoder()\n",
        "        self.pretrained_vit.vit.encoder.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, model_dim))\n",
        "        self.pretrained_vit.project = nn.Conv2d(out_channels * 8, 768, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "        self.linear_vit = Linear_ViT_Encoder(self.model_dim, num_patches, block_num=5)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(model_dim, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x1 = self.relu(x)\n",
        "        # print('This is the shape of x1:', x1.shape)\n",
        "\n",
        "        x2 = self.encoder1(x1)\n",
        "        # print('This is the shape of x2:',x2.shape)\n",
        "        x3 = self.encoder2(x2)\n",
        "        # print('This is the shape of x3:',x3.shape)\n",
        "        x = self.encoder3(x3)\n",
        "        # print('This is the shape of x:',x.shape)\n",
        "\n",
        "        x = self.pretrained_vit(x)\n",
        "        # print('After Pretrained layer', x.shape)\n",
        "        x = self.linear_vit(x)\n",
        "        e = f = int(self.num_patches ** 0.5)\n",
        "        x = rearrange(x, \"b (e f) c -> b c e f\", e = e, f = f)\n",
        "        # print('After Rearrange', x.shape)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        # print('After Conv',x.shape)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x, x1, x2, x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, skip_channels, scale_factor=2, size=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.skip_channels = skip_channels\n",
        "        self.scale_factor = scale_factor\n",
        "        self.size = size\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels + self.skip_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, x_concat=None):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=True)\n",
        "\n",
        "        if x_concat is not None:\n",
        "            x = torch.cat([x_concat, x], dim=1)\n",
        "\n",
        "        x = self.layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransUNetDecoder(nn.Module):\n",
        "    def __init__(self, out_channels, class_num=1, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "        # Define skip connection channels\n",
        "        skip_channels_list = [256, 128, 64]\n",
        "\n",
        "        self.decoder1 = DecoderBottleneck(out_channels*8,out_channels * 2+skip_channels_list[0],skip_channels=256)\n",
        "        self.decoder2 = DecoderBottleneck(out_channels*2+skip_channels_list[0], out_channels+skip_channels_list[1],skip_channels=128)\n",
        "        self.decoder3 = DecoderBottleneck(out_channels+skip_channels_list[1],int(out_channels * 1 / 2),skip_channels=64)\n",
        "\n",
        "        # The last decoder does have a skip connection\n",
        "        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 4), skip_channels=0)\n",
        "\n",
        "        # Segmentation layer for segmentation\n",
        "        self.seg_layer = nn.Conv2d(int(out_channels * 1 /4), class_num, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, x1, x2, x3, deep_sup=False):\n",
        "        if deep_sup:\n",
        "            x3 = F.interpolate(x3, scale_factor=self.scale_factor, mode='nearest')\n",
        "            x = self.decoder1(x, x3)\n",
        "            x2 = F.interpolate(x2, scale_factor=self.scale_factor, mode='nearest')\n",
        "            x = self.decoder2(x, x2)\n",
        "            x1 = F.interpolate(x1, scale_factor=self.scale_factor, mode='nearest')\n",
        "            x = self.decoder3(x, x1)\n",
        "            return x\n",
        "        else:\n",
        "            # print(\"Input shape to decoder1:\", x.shape)\n",
        "            # print('x3 shape for skip connection',x3.shape)\n",
        "            x = self.decoder1(x, x3)\n",
        "            # print('d1', x.shape)\n",
        "            x = self.decoder2(x, x2)\n",
        "            # print('d2',x.shape)\n",
        "            x = self.decoder3(x, x1)\n",
        "            # print('d3',x.shape)\n",
        "            x = self.decoder4(x)\n",
        "            # print('d4',x.shape)\n",
        "            x = self.seg_layer(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whole model would be defined here\n",
        "\n",
        "class TransUNet(nn.Module):\n",
        "    def __init__(self, in_channels = 1, encoder_out_channels = 512, decoder_out_channels = 64, head_num = 8, model_dim = 768, mlp_dim = 1536, block_num = 5, num_patches = 1024, class_num = 1):\n",
        "        super().__init__()\n",
        "        self.decoder_out_channels = decoder_out_channels\n",
        "\n",
        "        self.encoder = TransUNetEncoder(in_channels=1, out_channels=64, num_patches=1024, model_dim=768)\n",
        "        # self.encoder = TransUNetEncoder(in_channels, encoder_out_channels, model_dim, block_num, num_patches, num_heads = 8)\n",
        "\n",
        "        # self.decoder = TransUNetDecoder(out_channels = self.decoder_out_channels, class_num = 3)\n",
        "        self.decoder = TransUNetDecoder(out_channels=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x1, x2, x3 = self.encoder(x)\n",
        "        # print('x.shape:', x.shape)\n",
        "        # print('x1.shape:', x1.shape)\n",
        "        # print('x2.shape:', x2.shape)\n",
        "        # print('x3.shape:', x3.shape)\n",
        "        x = self.decoder(x, x1, x2, x3)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize loss functions\n",
        "# Those special loss functions are defined in segmentation_models_pytorch, just use them directly\n",
        "JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n",
        "DiceLoss = smp.losses.DiceLoss(mode='multilabel')\n",
        "BCELoss = smp.losses.SoftBCEWithLogitsLoss()\n",
        "LovaszLoss = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n",
        "TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
        "\n",
        "def dice_coef(y_true: torch.Tensor, y_pred: torch.Tensor, thr: float = 0.5, dim: tuple = (1,2), epsilon: float = 0.001) -> torch.Tensor:\n",
        "    \"\"\"Compute the Dice coefficient.\"\"\"\n",
        "    y_true = y_true.to(torch.float32)\n",
        "    y_pred = (y_pred > thr).to(torch.float32)\n",
        "    inter = (y_true * y_pred).sum(dim=dim)\n",
        "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n",
        "    dice = ((2 * inter + epsilon) / (den + epsilon)).mean(dim=(0))\n",
        "    return dice\n",
        "\n",
        "def iou_coef(y_true: torch.Tensor, y_pred: torch.Tensor, thr: float = 0.5, dim: tuple = (1,2), epsilon: float = 0.001) -> torch.Tensor:\n",
        "    \"\"\"Compute the Intersection over Union (IoU) coefficient.\"\"\"\n",
        "    y_true = y_true.to(torch.float32)\n",
        "    y_pred = (y_pred > thr).to(torch.float32)\n",
        "    inter = (y_true * y_pred).sum(dim=dim)\n",
        "    union = (y_true + y_pred - y_true * y_pred).sum(dim=dim)\n",
        "    iou = ((inter + epsilon) / (union + epsilon)).mean(dim=(0))\n",
        "    return iou\n",
        "\n",
        "def criterion(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Compute the combined loss as a weighted sum of BCE and Tversky losses.\"\"\"\n",
        "    return 0.5 * BCELoss(y_pred, y_true) + 0.5 * TverskyLoss(y_pred, y_true)\n",
        "\n",
        "def fetch_scheduler(optimizer: torch.optim.Optimizer) -> lr_scheduler._LRScheduler:\n",
        "    \"\"\"Fetch the learning rate scheduler based on the configuration.\"\"\"\n",
        "    if CFG.lr_scheduler == 'CosineAnnealingLR':\n",
        "        return lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.lr_min)\n",
        "    elif CFG.lr_scheduler == 'CosineAnnealingWarmRestarts':\n",
        "        return lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, eta_min=CFG.lr_min)\n",
        "    elif CFG.lr_scheduler == 'ReduceLROnPlateau':\n",
        "        return lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, threshold=0.0001, min_lr=CFG.lr_min)\n",
        "    elif CFG.lr_scheduler == 'ExponentialLR':\n",
        "        return lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
        "    elif CFG.lr_scheduler is None:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRe6s9ntfCIm"
      },
      "source": [
        "**Define the model training phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model: torch.nn.Module, \n",
        "                    optimizer: torch.optim.Optimizer, \n",
        "                    scheduler, \n",
        "                    dataloader: torch.utils.data.DataLoader, \n",
        "                    device: torch.device, \n",
        "                    epoch: int, \n",
        "                    criterion) -> float:\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The model to train.\n",
        "    - optimizer: The optimizer to use.\n",
        "    - scheduler: The learning rate scheduler.\n",
        "    - dataloader: The data loader for training data.\n",
        "    - device: The device to train on (e.g., 'cuda').\n",
        "    - epoch: The current epoch number.\n",
        "\n",
        "    Returns:\n",
        "    - The average loss for the epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
        "    for step, data in pbar:\n",
        "        images = data['image'] \n",
        "        masks = data['label']\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        masks = masks.to(device, dtype=torch.float)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = model(images)\n",
        "        y_pred = y_pred.squeeze(1)\n",
        "        loss = criterion(y_pred, masks)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        running_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
        "                         lr=f'{current_lr:0.5f}',\n",
        "                         gpu_mem=f'{mem:0.2f} GB')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5LTLgbfXS2"
      },
      "source": [
        "**Define the model validation phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test time augmentation would be used here\n",
        "\n",
        "def tta_inference(model, images):\n",
        "    \"\"\"Apply Test-Time Augmentation and return averaged predictions.\"\"\"\n",
        "    # Original\n",
        "    preds = model(images)\n",
        "\n",
        "    # Horizontal Flip\n",
        "    preds_hflip = model(torch.flip(images, [3]))\n",
        "\n",
        "    # Vertical Flip\n",
        "    preds_vflip = model(torch.flip(images, [2]))\n",
        "\n",
        "    # Average predictions\n",
        "    preds = (preds + preds_hflip + preds_vflip) / 3.0\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def valid_one_epoch(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, device: torch.device, optimizer) -> tuple:\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0.0\n",
        "    val_scores = []\n",
        "    \n",
        "    # pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in pbar:\n",
        "            masks = data['label'].to(device, dtype=torch.float)\n",
        "\n",
        "            for channel in range(data['image'].shape[1]):\n",
        "                image = data['image'][:, channel:channel+1].to(device, dtype=torch.float)  # Extract a single channel\n",
        "\n",
        "                batch_size = image.size(0)\n",
        "\n",
        "                y_pred = tta_inference(model, image)\n",
        "                y_pred = y_pred.squeeze(1)\n",
        "                loss = criterion(y_pred, masks)\n",
        "\n",
        "                running_loss += (loss.item() * batch_size)\n",
        "                dataset_size += batch_size\n",
        "\n",
        "                epoch_loss = running_loss / dataset_size\n",
        "\n",
        "                y_pred = nn.Sigmoid()(y_pred)\n",
        "                val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n",
        "                val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n",
        "                val_scores.append([val_dice, val_jaccard])\n",
        "\n",
        "                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}', lr=f'{current_lr:0.5f}', gpu_memory=f'{mem:0.2f} GB')\n",
        "\n",
        "    val_scores = np.mean(val_scores, axis=0)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss, val_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my-D7rCXfs2F"
      },
      "source": [
        "**Save the best parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlLsjdK9fnjO"
      },
      "outputs": [],
      "source": [
        "def save_model(model, filename, is_best=False):\n",
        "    \"\"\"Save the model's state dict to a file.\"\"\"\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    if is_best:\n",
        "        wandb.save(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecgZka-fgCl2"
      },
      "source": [
        "**Define Run Training function here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-wvNVkfw7F"
      },
      "outputs": [],
      "source": [
        "def run_training(model: torch.nn.Module, optimizer: torch.optim.Optimizer, scheduler, device: torch.device, num_epochs: int) -> tuple:\n",
        "    \"\"\"\n",
        "    Train the model for a specified number of epochs and log metrics to wandb.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The model to train.\n",
        "    - optimizer: The optimizer to use.\n",
        "    - scheduler: The learning rate scheduler.\n",
        "    - device: The device to train on (e.g., 'cuda').\n",
        "    - num_epochs: The number of epochs to train.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple containing the trained model and training history.\n",
        "    \"\"\"\n",
        "    wandb.watch(model, log_freq=100)\n",
        "    \n",
        "    # ema = EMA(model)\n",
        "    \n",
        "    # Test if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"cuda: {torch.cuda.get_device_name()}\\n\")\n",
        "     \n",
        "    start_time = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_dice = -np.inf\n",
        "    best_epoch = -1\n",
        "    history = defaultdict(list)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    run = wandb.init(\n",
        "        project='uw-maddison-gi-tract',\n",
        "        config={k: v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
        "        anonymous=CFG.anonymous,\n",
        "        name=f\"dim-{CFG.image_size[0]}x{CFG.image_size[1]}|model-{CFG.model_name}\",\n",
        "        group=CFG.comment\n",
        "    )\n",
        "    \n",
        "    train_loader, valid_loader, test_loader = prepare_synapse_loaders(base_dir=r\"project_TransUNet/data/Synapse/train_npz\",\n",
        "                                                                      list_dir=r\"project_TransUNet/TransUNet/lists/lists_Synapse\", debug=False)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        gc.collect()\n",
        "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
        "\n",
        "        train_loss = train_one_epoch(model, optimizer, scheduler, criterion = criterion, dataloader=train_loader, device=device, epoch=epoch)\n",
        "        val_loss, val_scores = valid_one_epoch(model, dataloader= valid_loader, device=device, optimizer = optimizer)\n",
        "        val_dice, val_jaccard = val_scores\n",
        "\n",
        "        history['Train Loss'].append(train_loss)\n",
        "        history['Valid Loss'].append(val_loss)\n",
        "        history['Valid Dice'].append(val_dice)\n",
        "        history['Valid Jaccard'].append(val_jaccard)\n",
        "\n",
        "        # Log the metrics\n",
        "        wandb.log({\"Train Loss\": train_loss,\n",
        "                   \"Valid Loss\": val_loss,\n",
        "                   \"Valid Dice\": val_dice,\n",
        "                   \"Valid Jaccard\": val_jaccard,\n",
        "                   \"LR\":scheduler.get_last_lr()[0]})\n",
        "\n",
        "        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n",
        "\n",
        "        # deep copy the model\n",
        "        if val_dice >= best_dice:\n",
        "            print(f\"Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n",
        "            best_dice    = val_dice\n",
        "            best_jaccard = val_jaccard\n",
        "            best_epoch   = epoch\n",
        "            run.summary[\"Best Dice\"]    = best_dice\n",
        "            run.summary[\"Best Jaccard\"] = best_jaccard\n",
        "            run.summary[\"Best Epoch\"]   = best_epoch\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            PATH = f\"best_epoch.bin\"\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            # Save a model file from the current directory\n",
        "            wandb.save(PATH)\n",
        "            print(f\"Model Saved\")\n",
        "\n",
        "        last_model_wts = copy.deepcopy(model.state_dict())\n",
        "        PATH = f\"last_epoch.bin\"\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "\n",
        "        print(); print()\n",
        "\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f'Training complete in {hours:.0f}h {minutes:.0f}m {seconds:.0f}s')\n",
        "    print(f\"Best Score: {best_jaccard:.4f}\")\n",
        "\n",
        "    wandb.log({\"Training Time\": elapsed_time})\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6v3N98guPU"
      },
      "source": [
        "**Define Loss Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIymXhhThKN-"
      },
      "source": [
        "**Helper function for loading the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiF_C0A_gyRH"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = TransUNet(in_channels=1, encoder_out_channels=512, decoder_out_channels = 64, head_num=8, model_dim=768, mlp_dim=2048, block_num=5, num_patches=1024, class_num=1)\n",
        "    model.to(CFG.device)\n",
        "    return model\n",
        "\n",
        "def load_model(path):\n",
        "    model = build_model()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzR0cdVchabP"
      },
      "source": [
        "**Prepare to train our model!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TransUNet(in_channels=1, encoder_out_channels=512, decoder_out_channels = 64, head_num=8, model_dim=768, mlp_dim=2048, block_num=5, num_patches=1024, class_num=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpYZBjEZhNif"
      },
      "outputs": [],
      "source": [
        "# model = build_model()\n",
        "# Initialize EMA\n",
        "\n",
        "# ema = EMA(model)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-6, weight_decay=CFG.weight_decay)\n",
        "scheduler = fetch_scheduler(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qJBAQH3h_JW"
      },
      "source": [
        "**Train the model!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3L6pcr6hlNG"
      },
      "outputs": [],
      "source": [
        "def train_model(CFG, model, optimizer, scheduler, device=CFG.device, num_epochs=CFG.epochs):\n",
        "    print(f\"\\n{'#' * 15}\\n### Training Model\\n{'#' * 15}\\n\")\n",
        "\n",
        "    # Initialize Weights & Biases run\n",
        "    run = wandb.init(\n",
        "        project='uw-maddison-gi-tract',\n",
        "        config={k: v for k, v in dict(vars(CFG)).items() if '__' not in k},\n",
        "        anonymous=CFG.anonymous,\n",
        "        name=f\"dim-{CFG.image_size[0]}x{CFG.image_size[1]}|model-{CFG.model_name}\",\n",
        "        group=CFG.comment\n",
        "    )\n",
        "\n",
        "    # Prepare data loaders\n",
        "    # train_loader, valid_loader, test_loader = prepare_synapse_loaders(base_dir=r\"C:\\Users\\ra78lof\\Desktop\\TransUNet\\project_TransUNet\\data\\Synapse\\train_npz\",\n",
        "                                                                      #list_dir=r\"C:\\Users\\ra78lof\\Desktop\\TransUNet\\project_TransUNet\\TransUNet\\lists\\lists_Synapse\", debug=False)\n",
        "    \n",
        "    # Train the model\n",
        "    model, history = run_training(model, optimizer, scheduler, device=CFG.device, num_epochs=CFG.epochs)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "    # Display the Weights & Biases dashboard\n",
        "    # display(display.IFrame(run.url, width=1000, height=720))\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqGkQEnqiEAH"
      },
      "source": [
        "**Test the model!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRZ7-NkRiDge"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, config):\n",
        "    # Prepare test dataset and loader\n",
        "    _, _, test_loader = prepare_synapse_loaders(base_dir=r\"project_TransUNet/data/Synapse/train_npz\", \n",
        "                                                list_dir=r\"project_TransUNet/TransUNet/lists/lists_Synapse\", debug=False)\n",
        "    imgs = next(iter(test_loader)).to(CFG.device, dtype=torch.float)\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        pred = model(imgs)\n",
        "        pred = (nn.Sigmoid()(pred) > 0.5).double()\n",
        "        preds.append(pred)\n",
        "\n",
        "    return torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_model, training_history = train_model(CFG, model, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.datasets import VOCDetection\n",
        "\n",
        "voc_dataset = VOCDetection(root=\"data\", year=\"2012\", image_set=\"trainval\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomVOCDetection(Dataset):\n",
        "    def __init__(self, voc_dataset, transforms=None):\n",
        "        self.voc_dataset = voc_dataset\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.voc_dataset[idx]\n",
        "        boxes = target[\"annotation\"][\"object\"]\n",
        "        labels = []\n",
        "        bboxes = []\n",
        "        for box in boxes:\n",
        "            xmin = float(box[\"bndbox\"][\"xmin\"])\n",
        "            ymin = float(box[\"bndbox\"][\"ymin\"])\n",
        "            xmax = float(box[\"bndbox\"][\"xmax\"])\n",
        "            ymax = float(box[\"bndbox\"][\"ymax\"])\n",
        "            bboxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(1)  # assuming all objects are of the same class\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=img, bboxes=bboxes, labels=labels)\n",
        "            img = transformed[\"image\"]\n",
        "            bboxes = transformed[\"bboxes\"]\n",
        "\n",
        "        return img, bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's Go!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
